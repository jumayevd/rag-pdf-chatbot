import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
import gradio as gr
import os
from dotenv import load_dotenv
load_dotenv()
# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

# Set your Google API Key here
GOOGLE_API_KEY = os.environ["GOOGLE_API_KEY"] 

## LLM
def get_llm():
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        temperature=0.3,
        max_tokens=512,
        google_api_key=GOOGLE_API_KEY
    )
    return llm

## Document loader
def document_loader(file):
    loader = PyPDFLoader(file.name)
    loaded_document = loader.load()
    return loaded_document

## Text splitter
def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
    )
    chunks = text_splitter.split_documents(data)
    return chunks

## Embedding model (using local/free HuggingFace embeddings)
def get_embeddings():
    # Using a small, fast, and completely free local embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    return embeddings

## Vector database
def vector_database(chunks):
    embedding_model = get_embeddings()
    vectordb = Chroma.from_documents(chunks, embedding_model)
    return vectordb

## Retriever
def retriever(file):
    splits = document_loader(file)
    chunks = text_splitter(splits)
    vectordb = vector_database(chunks)
    retriever = vectordb.as_retriever(search_kwargs={"k": 3})
    return retriever

## QA Chain
def retriever_qa(file, query):
    if not file:
        return "Please upload a PDF file first."
    
    if not query.strip():
        return "Please enter a question."
    
    try:
        llm = get_llm()
        retriever_obj = retriever(file)
        
        # Custom prompt template for better responses
        template = """Use the following pieces of context to answer the question at the end. 
        If you don't know the answer based on the context, just say that you don't know, don't try to make up an answer.
        Keep the answer concise and relevant.

        Context:
        {context}

        Question: {question}
        
        Answer:"""
        
        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "question"],
            template=template,
        )
        
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever_obj,
            return_source_documents=True,
            chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
        )
        
        response = qa.invoke({"query": query})
        return response['result']
    
    except Exception as e:
        return f"Error processing your request: {str(e)}"

# Create Gradio interface
rag_application = gr.Interface(
    fn=retriever_qa,
    allow_flagging="never",
    inputs=[
        gr.File(label="Upload PDF File", file_count="single", file_types=['.pdf'], type="filepath"),
        gr.Textbox(label="Ask a Question", lines=2, placeholder="What would you like to know about this document?")
    ],
    outputs=gr.Textbox(label="Answer", lines=5),
    title="ðŸ“š PDF Chat with Gemini (Free Version)",
    description="Upload a PDF document and ask questions about it. Uses Google Gemini for answers and local embeddings (no quota limits).",
    examples=[
        [None, "What is the main topic of this document?"],
        [None, "Can you summarize the key points?"],
        [None, "What are the conclusions mentioned?"]
    ],
    theme=gr.themes.Soft()
)

# Launch the app
if __name__ == "__main__":
    rag_application.launch(share=True, server_name="0.0.0.0", server_port=8890)